{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7073806,"sourceType":"datasetVersion","datasetId":4021394},{"sourceId":7155205,"sourceType":"datasetVersion","datasetId":4132031}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch #ML\nimport torch.nn.functional as F\nfrom torch.autograd import Variable #these will help us define the prefixes we will optimize\nimport numpy as np #math\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel #GPT-2 XL and its tokenizer\nfrom tqdm import tqdm #progress bar\nimport csv #reading the CSV\nfrom nltk.translate.bleu_score import sentence_bleu #BLEU score computation\ndevice=\"cuda\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T09:24:32.886836Z","iopub.execute_input":"2023-12-15T09:24:32.887251Z","iopub.status.idle":"2023-12-15T09:24:32.893066Z","shell.execute_reply.started":"2023-12-15T09:24:32.887222Z","shell.execute_reply":"2023-12-15T09:24:32.892109Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl') #tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device) #model\n\nvocab_len, embed_size = tuple(model.state_dict()['transformer.wte.weight'].shape)\nembedding_matrix = model.state_dict()['transformer.wte.weight'].clone().cpu() #actual embedding matrix\n\n#we create a custom tokenizer function here, because we want to return an array of embeddings rather than \n#an array of indices\ndef tokenize(string):\n    \"\"\"\n    Tokenizes a string and converts it to a tensor of embeddings.\n    \n    Args:\n    string (str): The input string to be tokenized.\n    \n    Returns:\n    A tensor of embeddings for the input string.\n    \"\"\"\n    # Tokenize the string using the tokenizer\n    x = torch.tensor(tokenizer(string)['input_ids']).view(1, -1)\n    \n    # Compute the prompt length and embeddings\n    prompt_len = x.shape[-1]\n    prompt_embeddings = F.one_hot(x, num_classes=vocab_len).float() @ embedding_matrix\n    \n    # Return the prompt embeddings on the device\n    return prompt_embeddings.to(device)\n\n# Assuming start_english_prompt and start_french_prompt are the variables you want to save\n\n# Save the variables to a file\nstart_english_prompt = torch.load('/kaggle/input/eng-fr-prefixes/start_english_prompt tuning_length32.pth')\nstart_french_prompt = torch.load('/kaggle/input/eng-fr-prefixes/start_french_prompt tuning_length32.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:24:32.894667Z","iopub.execute_input":"2023-12-15T09:24:32.894967Z","iopub.status.idle":"2023-12-15T09:24:49.722584Z","shell.execute_reply.started":"2023-12-15T09:24:32.894942Z","shell.execute_reply":"2023-12-15T09:24:49.721595Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def create_example(lang1_str, lang2_str, start_lang1_prompt, start_lang2_prompt):\n    \"\"\"\n    Concatenates the prefixes and the two input language strings and returns the resulting tensor and the\n    target ground-truth tensor for the second language string.\n    \n    Args:\n    lang1_str (str): The first language string.\n    lang2_str (str): The second language string.\n    start_lang1_prompt (torch.Tensor): The tensor representing the prefix for the first language string.\n    start_lang2_prompt (torch.Tensor): The tensor representing the prefix for the second language string.\n    \n    Returns:\n    A tuple containing the concatenated tensor of the prefixes and the two input language strings, and \n    the ground truth tensor for training.\n    \"\"\"\n    lang1 = tokenize(lang1_str)\n    lang2 = tokenize(lang2_str)\n        \n    out = torch.concat((start_lang1_prompt, lang1, start_lang2_prompt, lang2), dim=1)\n    \n    #we had an EOS token to the target so that we can know when the translation is done generating\n    return out, torch.tensor(tokenizer(lang2_str + \"<|endoftext|>\")['input_ids']).view(1, -1)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:24:49.723882Z","iopub.execute_input":"2023-12-15T09:24:49.724266Z","iopub.status.idle":"2023-12-15T09:24:49.730736Z","shell.execute_reply.started":"2023-12-15T09:24:49.724227Z","shell.execute_reply":"2023-12-15T09:24:49.729757Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def translate_argmax(start_lang1_prompt, input_str, start_lang2_prompt, verbose=True):\n    \"\"\"\n    Translates an input language string to the target language using the argmax method.\n    \n    Args:\n    start_lang1_prompt (torch.Tensor): The tensor representing the prefix for the input language.\n    input_str (str): The input language string to be translated.\n    start_lang2_prompt (torch.Tensor): The tensor representing the prefix for the target language.\n    verbose (bool): If True, the function prints the predicted tokens as they are generated. Defaults to True.\n    \n    Returns:\n    A list of token IDs representing the translated target language string.\n    \"\"\"\n    lang1 = tokenize(input_str) #tokenize input\n    out = torch.concat((start_lang1_prompt, lang1, start_lang2_prompt), dim=1) #create input prompt\n    input_len = out.shape[1] #get the input length\n    \n    if input_len > 512: #memory constraints :(\n        return 0\n    \n    with torch.no_grad(): #use no_grad to save memory\n        out_sequence = [] #will be our output sequence\n        \n        #generate our first output token\n        out = model(inputs_embeds=out.to(device))\n        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n        \n        if verbose:\n            print(tokenizer.decode(out_sequence[-1]), end='')\n        \n        #we terminate generation either when we see the EOS token, *or* when the length of the generation \n        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n        #the input sequence. This last termination clause is just to not waste our time when the model \n        #gets stuck on some repetitive or non-sense generation.\n        while out_sequence[-1] != 50256 and len(out_sequence) + input_len < min(1024, input_len*2):\n            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n                        past_key_values=out['past_key_values']) #use KV recycling to save compute!\n            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n            if verbose:\n                if out_sequence[-1] != 50256:\n                    print(tokenizer.decode(out_sequence[-1]), end='')\n    \n    return out_sequence[:-1] #return all but the EOS token\n\ndef en_fr_3_shot_argmax(input_str, verbose=True):\n    \"\"\"\n    Translates a given English input string to French using a 3-shot prompt.\n    \n    Args:\n    input_str (str): The input English string to be translated.\n    verbose (bool, optional): If True, the function will print the translation as it is being generated. Defaults to True.\n    \n    Returns:\n    out_sequence (list): A list of tokens representing the generated French translation.\n    \"\"\"\n    out = tokenize(prompt_3_shot_en_fr + \" \" + input_str.strip() + \"\\nfr:\") #prepare input prompt\n    input_len = out.shape[1] #get the length of the input\n    \n    if input_len > 512: #memory constraints :(\n        return 0\n    \n    with torch.no_grad(): #save memory with no_grad\n        out_sequence = []\n        out = model(inputs_embeds=out.to(device)) #sample first token\n        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n        \n        if verbose:\n            print(tokenizer.decode(out_sequence[-1]), end='')\n        \n        #we terminate generation either when we see a newline token, *or* when the length of the generation \n        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n        #the input sequence. This last termination clause is just to not waste our time when the model \n        #gets stuck on some repetitive or non-sense generation.\n        while out_sequence[-1] != 198 and len(out_sequence) + input_len < min(1024, input_len*2):\n            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n                        past_key_values=out['past_key_values'])\n            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n            if verbose:\n                if out_sequence[-1] != 198:\n                    print(tokenizer.decode(out_sequence[-1]), end='')\n    \n    return out_sequence[:-1]\n\ndef fr_en_3_shot_argmax(input_str, verbose=True):\n    \"\"\"\n    Translates a given French input string to English using a 3-shot prompt.\n    \n    Args:\n    input_str (str): The input French string to be translated.\n    verbose (bool, optional): If True, the function will print the translation as it is being generated. Defaults to True.\n    \n    Returns:\n    out_sequence (list): A list of tokens representing the generated English translation.\n    \"\"\"\n    out = tokenize(prompt_3_shot_fr_en + \" \" + input_str.strip() + \"\\nen:\") #prepare input prompt \n    input_len = out.shape[1] #get the length of the input\n    \n    if input_len > 512: #memory constraints :(\n        return 0\n    \n    with torch.no_grad(): #save memory with no_grad\n        out_sequence = []\n        out = model(inputs_embeds=out.to(device)) #sample first token\n        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n        \n        if verbose:\n            print(tokenizer.decode(out_sequence[-1]), end='')\n        \n        #we terminate generation either when we see a newline token, *or* when the length of the generation \n        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n        #the input sequence. This last termination clause is just to not waste our time when the model \n        #gets stuck on some repetitive or non-sense generation.\n        while out_sequence[-1] != 198 and len(out_sequence) + input_len < min(1024, input_len*2):\n            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n                        past_key_values=out['past_key_values'])\n            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n            if verbose:\n                if out_sequence[-1] != 198:\n                    print(tokenizer.decode(out_sequence[-1]), end='')\n    \n    return out_sequence[:-1]","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:24:49.732681Z","iopub.execute_input":"2023-12-15T09:24:49.732985Z","iopub.status.idle":"2023-12-15T09:24:49.755418Z","shell.execute_reply.started":"2023-12-15T09:24:49.732959Z","shell.execute_reply":"2023-12-15T09:24:49.754497Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"translate_argmax(start_french_prompt, \"Je suis fute\", start_english_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:24:49.756435Z","iopub.execute_input":"2023-12-15T09:24:49.756670Z","iopub.status.idle":"2023-12-15T09:24:50.014640Z","shell.execute_reply.started":"2023-12-15T09:24:49.756648Z","shell.execute_reply":"2023-12-15T09:24:50.013895Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"I am fit","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"[40, 716, 4197]"},"metadata":{}}]}]}